---
title             : "NFC and burnout in teachers - A replication and extension study"
shorttitle        : "NFC and burnout in teachers"

author: 
  - name          : "Josephine Zerna"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "Zellescher Weg 17, 01069 Dresden, Germany"
    email         : "josephine.zerna@tu-dresden.de"
    role:         # Contributorship roles (e.g., CRediT, https://casrai.org/credit/)
      - Conceptualization
      - Methodology
      - Formal analysis
      - Writing - original draft preparation
      - Writing - review & editing
  - name          : "Nicole Engelmann"
    affiliation   : "2"
    role:
      - Conceptualization
      - Investigation
      - Writing - review & editing
  - name          : "Anja Strobel"
    affiliation   : "3"
    role:
      - Conceptualization
      - Supervision
      - Methodology
      - Writing - review & editing
  - name          : "Alexander Strobel"
    affiliation   : "1"
    role:
      - Conceptualization
      - Supervision
      - Methodology
      - Writing - review & editing

affiliation:
  - id            : "1"
    institution   : "Faculty of Psychology, Technische Universität Dresden"
  - id            : "2"
    institution   : "Faculty of Education, Technische Universität Dresden"
  - id            : "3"
    institution   : "Institute of Psychology, Chemnitz University of Technology"

authornote: |
  Add complete departmental affiliations for each author here. Each new line herein must be indented, like this line.

  Enter author note here.

abstract: |
  One or two sentences providing a **basic introduction** to the field,  comprehensible to a scientist in any discipline.
  
  Two to three sentences of **more detailed background**, comprehensible  to scientists in related disciplines.
  
  One sentence clearly stating the **general problem** being addressed by  this particular study.
  
  One sentence summarizing the main result (with the words "**here we show**" or their equivalent).
  
  Two or three sentences explaining what the **main result** reveals in direct comparison to what was thought to be the case previously, or how the  main result adds to previous knowledge.
  
  One or two sentences to put the results into a more **general context**.
  
  Two or three sentences to provide a **broader perspective**, readily comprehensible to a scientist in any discipline.
  
  <!-- https://tinyurl.com/ybremelq -->
  
keywords          : "mediation, resources, demands, structural equation modelling, Covid-19"
wordcount         : "X"

bibliography      : references.bib

floatsintext      : no
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "man"
output            : papaja::apa6_pdf

csl               : apa.csl # this is a workaround for a current bug in the papaja/Rmd software, which fails to shorten multiple authors to "et al."

header-includes   :
    - \usepackage{booktabs}
    - \usepackage{longtable}
    - \usepackage{array}
    - \usepackage{multirow}
    - \usepackage{wrapfig}
    - \usepackage{float}
    - \usepackage{colortbl}
    - \usepackage{pdflscape}
    - \usepackage{tabu}
    - \usepackage{threeparttable}
    - \usepackage{threeparttablex}
    - \usepackage[normalem]{ulem}
    - \usepackage{makecell}
    - \usepackage{xcolor}
---

```{r setup, include = FALSE}
library(papaja)
library(bibtex)

#install.packages("here")       # easy file referencing
#install.packages("tidyverse")  # plotting and managing data
#install.packages("lavaan")     # for structural equation modelling
#install.packages("psych")      # Cronbach's Alpha and such
#install.packages("MVN")        # Mardia's test and such
#install.packages("MBESS")      # MacDonald's Omega
#install.packages("Hmisc")      # for correlations with p-values
#install.packages("kableExtra") # for great tables (see haozhu233.github.io/kableExtra/awesome_table_in_pdf.pdf)
#install.packages("knitr")      # for knitting documents and tables
#install.packages("semPlot")    # plotting path models
#install.packages("remotes")    # for APA output (remotes::install_github("dgromer/apa"))

library(here)
library(tidyverse)
library(ggplot2)
library(lavaan)
library(psych)
library(MVN)
library(MBESS)
library(Hmisc)
library(kableExtra)
library(knitr)
library(GGally)                 # from ggplot2, for multi-correlation plots
library(semPlot)
library(qgraph)
library(apa)                    # from the remotes package

```

```{r analysis-preferences, echo=FALSE, message=FALSE, warning=FALSE}

  # Seed for random number generation
  set.seed(13)
  knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
  
  # Set a number for the bootstrapping procedures
  bootnum <- 5000

```

# Introduction

Need for Cognition (NFC) is a stable intrinsic motivation to seek out and especially to enjoy effortful cognitive activities [@Cacioppo1982].
As it bridges the gap between cognition and motivation, NFC is considered to be an investment trait [@Stumm2013], and has come to the fore of psychological research in the last years.
NFC can easily be assessed using the Need for Cognition Scale (NCS), a self-report questionnaire with 18 to 34 items [@Cacioppo1982; @Cacioppo1984].
While many studies have found positive associations of NFC with academic performance [@Cazan2014; @Elias2002; @Grass2017; @Lavrijsen2021; @Zheng2020], recent investigations have also looked at NFC as a personal resource in academic and work contexts.
Individuals high in NFC have more positive emotions at the end of the work day [@Rosen2020], higher work motivation, perceive their roles as less ambiguous [@Nowlin2017], are less likely to drop out of college [@Grass2017; @Klaczynski1996], and have less anxiety regarding their course work [@Karagiannopoulou2020].
These findings suggest that individuals high in NFC might be less prone to experience adverse effects of work stress, which range from physical [@Dragano2017; @Steptoe2013] to psychological [@Madsen2017; @Maslach2016; @Wiesner2005].

One of these psychological consequences is burnout, a state of exhaustion and cynicism caused by long-term overstimulation in the workplace, which results in employees being dissatisfied, being sick more often, and performing poorly [@Schaufeli2014].
Burnout is especially prevalent in social jobs such as healthcare or teaching because the worker is always in conflict between advocating for their client and meeting the goals set by the employer [@GrayStanley2011; @Lloyd2002].
Lackritz [-@Lackritz2004] found that university teachers' burnout scores were higher the more students they had, the higher their teaching load was, and the more time they spent grading students' work.
Burnout is most often assessed using the Maslach Burnout Inventory (MBI) [@Maslach1997], a self-report questionnaire with three subscales: Emotional exhaustion, depersonalisation, and reduced self-efficacy.

Individuals with high burnout scores are often passive copers, high in neuroticism, low in self-esteem, and have an external locus of control [@Schaufeli2014].
NFC on the other hand is negatively associated with those variables [@Double2016; @Fleischhauer2019; @Ghorbani2004; @Grass2018; @Osberg1987], suggesting that people high in NFC are less prone to experience burnout.
This is supported by the findings that NFC is negatively associated with burnout scores in adults [@Fleischhauer2019], students [@Fleischhauer2019; @Naderi2018], and teacher trainees [@Grass2018].
However, the associations of NFC with the sum score and the subscales of the MBI are not always consistent between these studies.
This is likely not caused by inaccurate measurement, since the validity of both NCS [@Bless1994; @Osberg1987; @Tolentino1990] and MBI [@Brady 2021; @Kantas1997; @Schaufeli2001; @ValdiviaVazquez2021] has been demonstrated in multiple studies.
What is more likely is the influence of one or more other variables, moderating or mediating the association of NFC and burnout.
Grass et al. [-@Grass2018] investigated such a mediation and found that the relation of NFC and the MBI subscale reduced personal-efficacy was fully mediated by reappraisal, active and passive coping, but not by suppression or self-control.
Reappraisal and suppression are two emotion regulation strategies, which refer to the cognitive reassessment of a stressor and the inhibition of emotional reactions, respectively [@Gross1998a].
The findings by Grass et al. [-@Grass2018] suggest that individuals high in NFC experience a weaker decline in personal efficacy in response to long-term stress because they actively reassess the situation in a way that reinforces their sense of self-efficacy and don't avoid dealing with the stressor.
One goal of this paper is to replicate the findings of Grass et al. [-@Grass2018] using a multiple mediation model on cross-sectional self-report data of teachers.
We expect NFC to be negatively associated with reduced personal efficacy via higher reappraisal scores, but not via suppression, via self-control, or directly.

Furthermore, we would like to extend the analysis to other possible mediators.
These mediators are motivated by our own recent survey of the literature on NFC and wellbeing, which suggested that individuals high in NFC might not only have a high level of personal resources but also overestimate their own resources to a certain degree [@Zerna2021].
Only a balance of resources and demands results in personal wellbeing, while an imbalance threatens wellbeing, regardless of whether this imbalance is in favour of resources or demands [@Dodge2012].
Following the framework of Hobfoll [-@Hobfoll1989], resources can be objects with practical or status purpose, conditions like marriage or tenure, personality aspects like coping style, and energies such as time, money, or knowledge.
In the case of NFC, resources are from the categories personality and energies: Personality, because NFC is a trait, encompassing a curious, analytic, and passionate approach to challenges, and energies, because individuals high in NFC have been coping actively all their life, which enriches their level of experience and knowledge in approaching challenges [@Cacioppo1996].
These personal resources matter with regards to stress assessment (how the situation is appraised) and with regards to both coping and recovery [@Salanova2006].
We would therefore like to investigate whether the association of NFC and burnout is mediated by different ratios of demands and resources; demands that are too high to be dealt with using one’s personal resources (DTH), demands that are too low for one’s personal resources (DTL), and a balanced fit of demands and resources (DRF).
Using the same data as for the replication, we compute a structural equation model (SEM) to assess the influence of these mediators.
Since individuals high in NFC are confident in their abilities [@Bye2009; @Ghorbani2004; @Heppner1983; @Klaczynski1996], we expect NFC to be negatively associated with DTH, positively associated with DTL, and positively associated with DRF.
And since burnout results from constant unpleasant activation by high demands, we expect it to be positively associated with DTH and negatively associated with DRF.
However, we have no hypothesis regarding the association of DTL and burnout, because even though DTL is akin to the concept of boredom and the consequences of boredom and burnout are very similar, burnout is a state with even lower activation and even more negative affect than boredom [@Schaufeli2014].

*The Covid-19 pandemic has exacerbated the rising prevalence of burnout [@Froebe2021].*

# Methods

We report how we determined our sample size, all data exclusions (if any), all manipulations, and all measures in the study [@Simmons2012].
Our preregistration, the data, and the R Markdown document used to analyze the data and write this manuscript are available at [https://osf.io/36ep9/](https://osf.io/36ep9/).

```{r import, echo=FALSE, message=FALSE, warning=FALSE}
  # Define the top level as the one where this file lies
  # If the rest of the files follow the structure of the Github repository, everything will work
  # (Simply download the entire repository to any location on your computer)
  
  here::i_am("flag_top_level_NFCBurnout.txt")
  
  # Load the data file
  
  load(here("Data", "data.Rda"))
  
  # Load the internal consistencies (bootstrapping has been suppressed to save time on knitting)
  consistencies <- read_rds(here("Data", "consistencies.rds"))
  # create a vector of consistencies to be used as the diagonal in a table later
  consist_diag <- cbind(format(round(t(consistencies[2,2:11]), digits = 2), nsmall = 2), rep("(",10),
                        format(round(t(consistencies[5,2:11]), digits = 2), nsmall = 2), rep(")",10))
  # paste together to create the output "alpha(omega)"
  consist_diag <- apply(consist_diag[,c(1:4)], 1 ,paste, collapse = "")
  
```


## Participants
Teachers were recruited via social media, emails to colleagues of N.E. and to Saxon schools with the request to pass on the information.
All teachers were eligible, no payment was issued.
Of the $N=$ 278 participants, who started filling out the online survey, $N=$ `r nrow(data)` (`r round((table(data$gender)[1]/nrow(data))*100, digits = 1)`% female, aged 20 to 67 years) data sets were complete and those participants indicated to have answered truthfully.
All of them were currently teaching at a primary, secondary, comprehensive, or vocational school.
Data was collected between the 12th of June and the 24th of July 2020.
At this point, schools had been switching between digital and hybrid forms of teaching for at least three months due to the Covid-19 pandemic, causing additional stress for many teachers.

## Material
All questionnaires were used in their German form.
Burnout was assessed using the 21-item Maslach Burnout Inventory (MBI) [@Buessing1992], NFC using the 16-item Need for Cognition Scale (NCS) [@Bless1994], self-control using the 13-item Self-Control Scale (SCS) [@Bertrams2009a], reappraisal and suppression using the 10-item Emotion Regulation Questionnaire (ERQ) [@Abler2009], and work satisfaction using the Allgemeine Arbeitszufriedenheit questionnaire (AAZ) [@Fischer2014].
Eleven items were created to assess each participant's current burden by the Covid-19 pandemic, such as whether they belong to a risk group or whether they currently had a higher workload.
The Covid-19 items can be found in the **Supplementary Material**.
The survey also included the Subjective Wellbeing Index of the World Health Organization [@Braehler2007], which we will not analyze.
Due to a technical error during survey setup, the coping style data of the Erfurter Belastungsinventar [@BoehmKasper2001] cannot be used, so we cannot replicate the mediation of NFC and burnout by active and passive coping.

## Procedure
The questionnaires were provided online using SoSci Survey [@Leiner2019].
Participants were informed about aims and duration of the study and data security, then they provided demographic information, answered the questionnaires, and could optionally enter their email address to be informed about the results of the analysis of N.E.'s thesis.

## Data analysis
We used *R Studio* [@RCT2020; @RStudioTeam2020] with the main packages *lavaan* [@Rosseel2012] and *psych* [@Revelle2021] for all our analyses.
Data were checked for multivariate normality using Mardia's coefficient.
To account for non-linear relationships, correlations were computed using Spearman's rank coefficient rather than Pearson's product moment correlation.
Internal consistencies were assessed with Cronbach's Alpha and MacDonald's Omega.
Since Cronbach's Alpha has been criticized for being insensitive to violations of internal consistency [@Dunn2014; @Taber2018], the additional computation of MacDonald's Omega has the purpose of ensuring a more reliable estimation.

### *Multiple mediation model*
Items were reverse coded according to the scale manuals.
NFC and self-control were computed as the sum scores of the NCS and the SCS, respectively.
Reduced personal efficacy was computed using the sum of the MBI subscale, and reappraisal and suppression were computed using the sum of each ERQ subscale.
NFC was entered as the independent variable, having a direct and multiple indirect effects on MBI via self-control, reappraisal, and suppression as mediators.
Following Grass et al. [-@Grass2018], the results of the model were appraised by using $N$ = 2,000 bootstrap samples for confidence intervals.
Multiple indices were used to evaluate model fit as recommended by Hu and Bentler [-@Hu1999]: the Chi-square test statistic, which measures the fit compared to a saturated model, the Comparative Fit Index (CFI), which compares the fit to the baseline model, the Standardized Root Mean Square Residual (SRMR), which compares the residuals of the observed and predicted covariance matrix, and the Root Mean Square Error of Approximation (RMSEA), which does the same as the latter but takes degrees of freedom and model complexity into account.

### *Structural equation model*
All items, apart from those making up the demand-resource-ratios, were reverse coded according to the scale manuals.
The latent factor NFC was computed by subjecting the NCS items to a parcelling procedure following Grass et al. [-@Grass2019], a method that is used in SEM when only relations between but not within constructs are of interest.
Principal component analysis was used to determine the factor loadings of each NCS item onto the first component.
Then, the items were randomly divided into four parcels and the average item loading per parcel was computed.
This was repeated 10,000 times to find the parcelling choice with the smallest difference in average item loadings between parcels.
The latent factor MBI was computed using the three subscales as indicators.
For the demand-resource-ratios, we used three items from the work satisfaction scale each.
The latent factor DTH was indicated by items 4, 8, and 9, DTL by the recoded items 12, 26, and 27, and DRF by items 17, 22, and 24.
Model parameters were estimated using the maximum likelihood method with robust standard errors.
Model fit was evaluated by looking at the Chi-square test statistic, CFI, SRMR, and RMSEA.

### *Exploratory analysis*
We preregistered two exploratory analyses.
Firstly, we repeated the SEM with the subscale reduced personal efficacy in place of the MBI score, since this subscale has shown higher correlations with NFC than the other subscales [@Grass2018; @Naderi2018].
And secondly, we included a Covid-19 burden score into the SEM, computed as the sum of the Covid-19 items.

```{r splitandrecode, echo=FALSE, message=FALSE, warning=FALSE}

  # Put questionnaire data into different data frames for easy access
  
  mbi <- data[,grep("MBI", colnames(data))]   # Maslach Burnout Inventory
  erq <- data[,grep("ERQ", colnames(data))]   # Emotion Regulation Questionnaire
  scs <- data[,grep("SCS", colnames(data))]   # Self Control Scale
  ws <- data[,grep("WS", colnames(data))]     # Work Satisfaction
  ncs <- data[,grep("NCS", colnames(data))]   # Need for Cognition Scale
  ncs <- ncs[,-9]                             # remove the dummy item
  covb <- data[,grep("COV", colnames(data))]  # Covid Burden Items
  dth <- ws[,c(4,8,9)]                        # the Work Satisfaction items for the latent variable Demands Too High
  dtl <- ws[,c(12,26,27)]                     # the Work Satisfaction items for the latent variable Demands Too Low
  drf <- ws[,c(17,21,24)]                     # the Work Satisfaction items for the latent variable Demand-Resource-Fit
  
  # Create a list with keys for the questionnaire subscales/latent variables/item parcels
  
  keylist <- list(mbi_ee = c(5,7,10,11,14,18,19,20,21), mbi_dp = c(1,6,9,15,17), mbi_rpe = c(2,3,4,8,12,13,16),
                  erq_supp = c(2,4,6,9), erq_reap = c(1,3,5,7,8,10),
                  scs = c(2,3,4,5,6,7,8,10,11),
                  ncs = c(4,6,7,8,9,10,11,12,15,16),
                  ws = c(4,5,6,8,9,10,11,13,16,19,20,33))
  
  # Recode all the reverse coded items
  
  mbi[,keylist$mbi_rpe] <- 7 - mbi[,keylist$mbi_rpe]  # Recode from 1-6 to 6-1
  scs[,keylist$scs] <- 6 - scs[,keylist$scs]          # Recode from 1-5 to 5-1
  scs <- scs - 3                                      # Recode all from 1-5 to -2-+2
  ncs[,keylist$ncs] <- 8 - ncs[,keylist$ncs]          # Recode from 1-7 to 7-1
  ncs <- ncs - 4                                      # Recode all from 1-7 to -3-+3
  ws[,keylist$ws] <- 6 - ws[,keylist$ws]              # Recode from 1-5 to 5-1
  ws <- ws - 3                                        # Recode all from 1-5 to -2-+2
  covb[,c(2,3,4,9)] <- 4 - covb[,c(2,3,4,9)]          # Recode from 1-3 to 3-1
  covb[,8] <- 3 - covb[,8]                            # Recode from 1-2 to 2-1
  # The following recoding is opposite to what is described in the methods section of the paper, which
  # is due to the fact that the questionnaire is originally coded as 1 (false) to 5 (true), but it was
  # coded as 1 (true) to 5 (false) in the SoSciSurvey of the present study.
  dth <- 6 - dth                                      # Recode all from 1-5 to 5-1
  dth <- dth - 3                                      # Recode all from 1-5 to -2-+2
  dtl <- dtl - 3                                      # Recode all from 1-5 to -2-+2
  drf <- 6 - drf                                      # Recode all from 1-5 to 5-1
  drf <- drf -3                                       # Recode all from 1-5 to -2-+2

```

```{r parcelling, echo=FALSE, message=FALSE, warning=FALSE}

  # Parcelling preparation: Compute factor loadings on the first PCA component
  
  ncs_pca <- princomp(ncs, cor = FALSE, scores = TRUE)
  ncs_load <- ncs_pca$loadings[,1]
  
  # Parcelling function
  
  parcel.gen <- function(loadings, parcels = 4,seed = 242,iterations = 10000) {
    
    p = parcels                   # number of parcels
    l = length(loadings)          # number of items
    set.seed(seed)              # seed for reproducibility
    
    out = parcels = NULL            # set up containers
    for (k in 1:iterations) {
      
      s = sample(1:l,l,replace = F) # random item order
      m = matrix(s,ncol = 4)        # separate into parcels
      
      # temporarily convert item numbers per parcel to concatenated strings
      parcels.i = NULL
      for (i in 1:p) {
        parcels.i = c(parcels.i,paste0(sort(m[,i]),collapse = ","))
      }
      # write to container
      parcels = rbind(parcels,parcels.i)
      
      # compute average item loadings per parcel
      avg.h = colMeans(matrix(loadings[m],ncol = 4))
      
      # compute differences between all average item loadings
      h.diff = NULL
      for (i in 1:(p-1)) {
        for (j in (i+1):p) {
          h.diff = c(h.diff,abs(avg.h[i]-avg.h[j]))
        }
      }
      
      # sum up differences
      h.diff.sum = sum(h.diff)
      # write summed differences to container
      out = c(out,h.diff.sum)
    }
    
    # find minimum summed difference and generate matrix or respective items
    final.parcels = matrix(as.numeric(unlist(strsplit(parcels[which.min(out),],','))),ncol = 4)
    # obtain corresponding loadings per parcel
    final.h = matrix(loadings[final.parcels],ncol = 4)
    
    # return result as list
    return(list(items = final.parcels,loadings = final.h,avg.loadings = colMeans(final.h),summed.difference = min(out)))
    
  }
  
  ncs_parcels <- parcel.gen(loadings = ncs_load) # execute function
  
  # Add parcelling result to the key list
  
  keylist <- c(keylist, parcel1 = list(ncs_parcels$items[,1]), parcel2 = list(ncs_parcels$items[,2]),
               parcel3 = list(ncs_parcels$items[,3]), parcel4 = list(ncs_parcels$items[,4]))
```

```{r scores, echo=FALSE, message=FALSE, warning=FALSE}

# Create data frame to feed the values of every subject into
  
  score_data <- data.frame(id = c(1:180),
                           years = data$yearsteaching,
                           mbi = rep(0,180),
                           mbi_ee = rep(0,180), mbi_dp = rep(0,180), mbi_rpe = rep(0,180),
                           erq = rep(0,180),
                           erq_supp = rep(0,180), erq_reap = rep(0,180),
                           scs = rep(0,180),
                           ncs = rep(0,180),
                           nfc1 = rep(0,180), nfc2 = rep(0,180), nfc3 = rep(0,180), nfc4 = rep(0,180),
                           dth = rep(0,180),
                           dth1 = rep(0,180), dth2 = rep(0,180), dth3 = rep(0,180),
                           dtl = rep(0,180),
                           dtl1 = rep(0,180), dtl2 = rep(0,180), dtl3 = rep(0,180),
                           drf = rep(0,180),
                           drf1 = rep(0,180), drf2 = rep(0,180), drf3 = rep(0,180),
                           covb = rep(0,180))
  
  # Compute individual scores or (for the latent variable indicators) feed in raw values
  
  for (i in 1:nrow(data)) {
    score_data[i,c(3:ncol(score_data))] <- c(rowSums(mbi[i,]),
                                             rowSums(mbi[i,keylist$mbi_ee]), rowSums(mbi[i,keylist$mbi_dp]), rowSums(mbi[i,keylist$mbi_rpe]),
                                             rowSums(erq[i,]),
                                             rowSums(erq[i,keylist$erq_supp]), rowSums(erq[i,keylist$erq_reap]),
                                             rowSums(scs[i,]),
                                             rowSums(ncs[i,]),
                                             rowSums(ncs[i,keylist$parcel1]), rowSums(ncs[i,keylist$parcel2]), rowSums(ncs[i,keylist$parcel3]), rowSums(ncs[i,keylist$parcel4]),
                                             rowSums(dth[i,]),
                                             dth[i,1], dth[i,2], dth[i,3],
                                             rowSums(dtl[i,]),
                                             dtl[i,1], dtl[i,2], dtl[i,3],
                                             rowSums(drf[i,]),
                                             drf[i,1], drf[i,2], drf[i,3],
                                             rowSums(covb[i,]))
  }

  # This function was taken from an answer on a Stackoverflow question on August 30th 2021
  # https://stackoverflow.com/questions/35085261/how-to-use-loess-method-in-ggallyggpairs-using-wrap-function/35088740

  my_fn <- function(data, mapping, pts=list(), smt=list(), ...){
    ggplot(data = data, mapping = mapping, ...) +
      do.call(geom_point, pts) +
      do.call(geom_smooth, smt)
  }
  
  # Plot a big correlational matrix to check for non-linear associations and unexpected outliers
  multicorrplot <- ggpairs(score_data[,c("years","mbi","mbi_ee","mbi_dp","mbi_rpe","erq","erq_supp","erq_reap",
                                         "scs","ncs","dth","dtl","drf","covb")], lower = list(continuous = my_fn))

  # It looked like there was an outlier in the data, especially visible in the MBI data
  # Check for the outlier in particular, using the data involved in the mediation
  outlierplot <- outlier(score_data[ ,c(6,8:11)], plot = TRUE)
  
  # Remove participant 161 in a second version of the data frame (for the supplementary material analysis)
  score_data_no_outlier <- score_data[score_data$mbi < 100, ]
  
```

```{r descriptives, echo=FALSE, message=FALSE, warning=FALSE}

  # Tests for normal distribution

    # using Mardia's test for multivariate normal distribution
    # results will be fed into the desc_met data frame in the next step
  
    isnorm_test <- mvn(score_data[c("mbi","mbi_ee","mbi_dp","mbi_rpe","erq","erq_supp","erq_reap","scs","ncs","dth","dtl","drf","covb")],
                       mvnTest = "mardia", covariance = TRUE, desc = TRUE, univariateTest = "SW")  
  
  # Descriptive statistics
  
    # Categorical descriptives: data frame with percentages of responses per category
    
    desc_cat <- data.frame(answer = c(1,2,3,4,5),
                           age = c(nrow(data[data$age == 1,])/nrow(data), nrow(data[data$age == 2,])/nrow(data), nrow(data[data$age == 3,])/nrow(data),
                                   nrow(data[data$age == 4,])/nrow(data), nrow(data[data$age == 5,])/nrow(data)),
                           gender = c(nrow(data[data$gender == 1,])/nrow(data), nrow(data[data$gender == 2,])/nrow(data), nrow(data[data$gender == 3,])/nrow(data),0,0),
                           schooltype = c(nrow(data[data$schooltype == 1,])/nrow(data), nrow(data[data$schooltype == 2,])/nrow(data), nrow(data[data$schooltype == 3,])/nrow(data),
                                          nrow(data[data$schooltype == 4,])/nrow(data), nrow(data[data$schooltype == 5,])/nrow(data)))
    
    # Metric descriptives: data frame with descriptive values of questionnaires
    
    desc_met <- data.frame(Variable = c("MBI","MBI EE","MBI DP","MBI RPE","ERQ","ERQ S","ERQ R","SCS","NFC","DTH","DTL","DRF","COV"),
                           Minimum = sapply(score_data[,c("mbi","mbi_ee","mbi_dp","mbi_rpe","erq","erq_supp","erq_reap","scs","ncs","dth","dtl","drf","covb")], min),
                           Maximum = sapply(score_data[,c("mbi","mbi_ee","mbi_dp","mbi_rpe","erq","erq_supp","erq_reap","scs","ncs","dth","dtl","drf","covb")], max),
                           Mean = isnorm_test$Descriptives$Mean,
                           SD = isnorm_test$Descriptives$Std.Dev,
                           Normality = isnorm_test$univariateNormality$Normality,
                           Skewness = isnorm_test$Descriptives$Skew,
                           Kurtosis = isnorm_test$Descriptives$Kurtosis)
    # Drop the row names
    
    rownames(desc_met) <- NULL
    
    # Replace the capital YESes and NOs with small ones
    
    desc_met$Normality[grep("NO", desc_met$Normality)] <- "No"
    desc_met$Normality[grep("YES", desc_met$Normality)] <- "Yes"
    
    # Round the values
    
    desc_met[ ,c("Mean","SD","Skewness","Kurtosis")] <- round(desc_met[ ,c("Mean","SD","Skewness","Kurtosis")], digits = 2)
    
    # Put the information in a table
    
    met_table <-
      kbl(desc_met,
      booktabs = T, # no vertical lines
      escape = F, # correctly print special characters
      align = c("l",rep("r",7))) %>% # right aligns the text in every column except for the first
    kable_styling(latex_options = c("striped", "scale_down")) %>%
    footnote(general = "MBI = Maslach Burnout Inventory, MBI EE = Emotional exhaustion subscale, MBI DP = Depersonalisation subscale, MBI RPE = Reduced personal efficacy subscale, ERQ = Emotion Regulation Questionnaire, ERQ S = Suppression subscale, ERQ R = Reappraisal subscale, SCS = Self-Control Scale, NFC = Need for Cognition, DTH = Demands Too High, DTL = Demands Too Low, DRF = Demand-Resource-Fit, COV = Covid-19 Burden, SD = Standard deviation. \\\\textit{N} = 180.",
             escape = F, # correctly print special characters
             threeparttable = T) # line breaks in the footnote

```

```{r consistency, echo=FALSE, message=FALSE, warning=FALSE, eval=FALSE}

  # Tests for internal consistency

    # Create data frame to feed values into

    consistencies <- data.frame(value = c("lower CI alpha","raw alpha","upper CI alpha",
                                          "lower CI omega","raw omega","upper CI omega"))

    # Compute Cronbach's Alpha and MacDonald's Omega for questionnaires and demand-resource-ratios

    set.seed(13) # seed for reproducing the bootstrapping
    
    # Maslach Burnout Inventory
    
    cronalph <- alpha(mbi, check.keys = TRUE)
    macomega <- ci.reliability(data=mbi, type="omega", conf.level = 0.95, interval.type="bca", B=bootnum)
    consistencies$mbi <- round(c(cronalph$total$raw_alpha - 1.96 * cronalph$total$ase, cronalph$total$raw_alpha,
                                 cronalph$total$raw_alpha + 1.96 * cronalph$total$ase,
                                 macomega$ci.lower, macomega$est, macomega$ci.upper), digits = 2)
    
    # MBI emotional exhaustion
    
    cronalph <- alpha(subset(mbi, select = keylist$mbi_ee), check.keys = TRUE)
    macomega <- ci.reliability(data=subset(mbi, select = keylist$mbi_ee), type="omega", conf.level = 0.95, interval.type="bca", B=bootnum)
    consistencies$mbi_ee <- round(c(cronalph$total$raw_alpha - 1.96 * cronalph$total$ase, cronalph$total$raw_alpha,
                                 cronalph$total$raw_alpha + 1.96 * cronalph$total$ase,
                                 macomega$ci.lower, macomega$est, macomega$ci.upper), digits = 2)
    
    # MBI depersonalization
    
    cronalph <- alpha(subset(mbi, select = keylist$mbi_dp), check.keys = TRUE)
    macomega <- ci.reliability(data=subset(mbi, select = keylist$mbi_dp), type="omega", conf.level = 0.95, interval.type="bca", B=bootnum)
    consistencies$mbi_dp <- round(c(cronalph$total$raw_alpha - 1.96 * cronalph$total$ase, cronalph$total$raw_alpha,
                                 cronalph$total$raw_alpha + 1.96 * cronalph$total$ase,
                                 macomega$ci.lower, macomega$est, macomega$ci.upper), digits = 2)
    
    # MBI reduced personal efficacy
    
    cronalph <- alpha(subset(mbi, select = keylist$mbi_rpe), check.keys = TRUE)
    macomega <- ci.reliability(data=subset(mbi, select = keylist$mbi_rpe), type="omega", conf.level = 0.95, interval.type="bca", B=bootnum)
    consistencies$mbi_rpe <- round(c(cronalph$total$raw_alpha - 1.96 * cronalph$total$ase, cronalph$total$raw_alpha,
                                 cronalph$total$raw_alpha + 1.96 * cronalph$total$ase,
                                 macomega$ci.lower, macomega$est, macomega$ci.upper), digits = 2)
    
    # ERQ
    
    cronalph <- alpha(erq, check.keys = TRUE)
    macomega <- ci.reliability(data=erq, type="omega", conf.level = 0.95, interval.type="bca", B=bootnum)
    consistencies$erq <- round(c(cronalph$total$raw_alpha - 1.96 * cronalph$total$ase, cronalph$total$raw_alpha,
                                  cronalph$total$raw_alpha + 1.96 * cronalph$total$ase,
                                  macomega$ci.lower, macomega$est, macomega$ci.upper), digits = 2)
    
    # ERQ suppression
    
    cronalph <- alpha(subset(erq, select = keylist$erq_supp), check.keys = TRUE)
    macomega <- ci.reliability(data=subset(erq, select = keylist$erq_supp), type="omega", conf.level = 0.95, interval.type="bca", B=bootnum)
    consistencies$erq_supp <- round(c(cronalph$total$raw_alpha - 1.96 * cronalph$total$ase, cronalph$total$raw_alpha,
                                  cronalph$total$raw_alpha + 1.96 * cronalph$total$ase,
                                  macomega$ci.lower, macomega$est, macomega$ci.upper), digits = 2)
    
    # ERQ reappraisal
    
    cronalph <- alpha(subset(erq, select = keylist$erq_reap), check.keys = TRUE)
    macomega <- ci.reliability(data=subset(erq, select = keylist$erq_reap), type="omega", conf.level = 0.95, interval.type="bca", B=bootnum)
    consistencies$erq_reap <- round(c(cronalph$total$raw_alpha - 1.96 * cronalph$total$ase, cronalph$total$raw_alpha,
                                  cronalph$total$raw_alpha + 1.96 * cronalph$total$ase,
                                  macomega$ci.lower, macomega$est, macomega$ci.upper), digits = 2)
    
    # Self Control Scale
    
    cronalph <- alpha(scs, check.keys = TRUE)
    macomega <- ci.reliability(data=scs, type="omega", conf.level = 0.95, interval.type="bca", B=bootnum)
    consistencies$scs <- round(c(cronalph$total$raw_alpha - 1.96 * cronalph$total$ase, cronalph$total$raw_alpha,
                             cronalph$total$raw_alpha + 1.96 * cronalph$total$ase,
                             macomega$ci.lower, macomega$est, macomega$ci.upper), digits = 2)
    
    # Need for Cognition Scale
    
    cronalph <- alpha(ncs, check.keys = TRUE)
    macomega <- ci.reliability(data=ncs, type="omega", conf.level = 0.95, interval.type="bca", B=bootnum)
    consistencies$ncs <- round(c(cronalph$total$raw_alpha - 1.96 * cronalph$total$ase, cronalph$total$raw_alpha,
                             cronalph$total$raw_alpha + 1.96 * cronalph$total$ase,
                             macomega$ci.lower, macomega$est, macomega$ci.upper), digits = 2)
    
    # Covid burden
    
    cronalph <- alpha(covb, check.keys = TRUE)
    macomega <- ci.reliability(data=covb, type="omega", conf.level = 0.95, interval.type="bca", B=bootnum)
    consistencies$covb <- round(c(cronalph$total$raw_alpha - 1.96 * cronalph$total$ase, cronalph$total$raw_alpha,
                             cronalph$total$raw_alpha + 1.96 * cronalph$total$ase,
                             macomega$ci.lower, macomega$est, macomega$ci.upper), digits = 2)
    
    # Demands Too High
    
    cronalph <- alpha(dth, check.keys = FALSE)
    macomega <- ci.reliability(data=dth, type="omega", conf.level = 0.95, interval.type="bca", B=bootnum)
    consistencies$dth <- round(c(cronalph$total$raw_alpha - 1.96 * cronalph$total$ase, cronalph$total$raw_alpha,
                             cronalph$total$raw_alpha + 1.96 * cronalph$total$ase,
                             macomega$ci.lower, macomega$est, macomega$ci.upper), digits = 2)
    
    # Demands Too Low
    
    cronalph <- alpha(dtl, check.keys = FALSE)
    macomega <- ci.reliability(data=dtl, type="omega", conf.level = 0.95, interval.type="bca", B=bootnum)
    consistencies$dtl <- round(c(cronalph$total$raw_alpha - 1.96 * cronalph$total$ase, cronalph$total$raw_alpha,
                             cronalph$total$raw_alpha + 1.96 * cronalph$total$ase,
                             macomega$ci.lower, macomega$est, macomega$ci.upper), digits = 2)
    
    # Demand Resource Fit
    
    cronalph <- alpha(drf, check.keys = FALSE)
    macomega <- ci.reliability(data=drf, type="omega", conf.level = 0.95, interval.type="bca", B=bootnum)
    consistencies$drf <- round(c(cronalph$total$raw_alpha - 1.96 * cronalph$total$ase, cronalph$total$raw_alpha,
                             cronalph$total$raw_alpha + 1.96 * cronalph$total$ase,
                             macomega$ci.lower, macomega$est, macomega$ci.upper), digits = 2)
    
    remove(cronalph,macomega) # delete the temporary variable
    
```

```{r correlations, echo=FALSE, message=FALSE, warning=FALSE}
    
    # The following function for nicely formatted correlation output was taken from
    # https://paulvanderlaken.com/2020/07/28/publication-ready-correlation-matrix-significance-r/
    # on the 5th of August 2021

    correlation_matrix <- function(df,
                                   type = "spearman",
                                   digits = 3,
                                   decimal.mark = ".",
                                   use = "all",
                                   show_significance = TRUE,
                                   replace_diagonal = FALSE,
                                   replacement = ""){
      # check arguments
      stopifnot({
        is.numeric(digits)
        digits >= 0
        use %in% c("all", "upper", "lower")
        is.logical(replace_diagonal)
        is.logical(show_significance)
        is.character(replacement)
      })
      # we need the Hmisc package for this
      require(Hmisc)
      
      # retain only numeric and boolean columns
      isNumericOrBoolean = vapply(df, function(x) is.numeric(x) | is.logical(x), logical(1))
      if (sum(!isNumericOrBoolean) > 0) {
        cat('Dropping non-numeric/-boolean column(s):', paste(names(isNumericOrBoolean)[!isNumericOrBoolean], collapse = ', '), '\n\n')
      }
      df = df[isNumericOrBoolean]
      
      # transform input data frame to matrix
      x <- as.matrix(df)
      
      # run correlation analysis using Hmisc package
      correlation_matrix <- Hmisc::rcorr(x, type = type)
      R <- correlation_matrix$r # Matrix of correlation coeficients
      p <- correlation_matrix$P # Matrix of p-value 
      
      # transform correlations to specific character format
      Rformatted = formatC(R, format = 'f', digits = digits, decimal.mark = decimal.mark)
      
      # if there are any negative numbers, we want to put a space before the positives to align all
      if (sum(!is.na(R) & R < 0) > 0) {
        Rformatted = ifelse(!is.na(R) & R > 0, paste0(" ", Rformatted), Rformatted)
      }
    
      # add significance levels if desired
      if (show_significance) {
        # define notions for significance levels; spacing is important.
        stars <- ifelse(is.na(p), "", ifelse(p < .001, "***", ifelse(p < .01, "**", ifelse(p < .05, "*", ""))))
        Rformatted = paste0(Rformatted, stars)
      }
      
      # make all character strings equally long
      max_length = max(nchar(Rformatted))
      Rformatted = vapply(Rformatted, function(x) {
        current_length = nchar(x)
        difference = max_length - current_length
        return(paste0(x, paste(rep(" ", difference), collapse = ''), sep = ''))
      }, FUN.VALUE = character(1))
      
      # build a new matrix that includes the formatted correlations and their significance stars
      Rnew <- matrix(Rformatted, ncol = ncol(x))
      rownames(Rnew) <- colnames(Rnew) <- colnames(x)
      
      # replace undesired values
      if (use == 'upper') {
        Rnew[lower.tri(Rnew, diag = replace_diagonal)] <- replacement
      } else if (use == 'lower') {
        Rnew[upper.tri(Rnew, diag = replace_diagonal)] <- replacement
      } else if (replace_diagonal) {
        diag(Rnew) <- replacement
      }
      
      return(Rnew)
    }
    
    
    # correlate variables
    correlations <- correlation_matrix(score_data[c("mbi","mbi_ee","mbi_dp","mbi_rpe","erq","erq_supp","erq_reap",
                                                    "scs","ncs","dth","dtl","drf","covb")],
                                       type = "spearman",
                                       use = "lower",
                                       show_significance = FALSE,
                                       replace_diagonal = TRUE,
                                       replacement = "")
    
    # reorder the columns of the consistencies data frame to match the correlation columns
    consist_diag <- consistencies[c(colnames(correlations))]
    
    # create a vector of consistencies to be used as the diagonal in a table later
    consist_diag <- cbind(format(round(t(consist_diag[2,]), digits = 2), nsmall = 2), rep("(",10),
                          format(round(t(consist_diag[5,]), digits = 2), nsmall = 2), rep(")",10))
    
    # paste together to create the output "alpha(omega)"
    consist_diag <- apply(consist_diag[,c(1:4)], 1 ,paste, collapse = "")
    
    # add previously assembled diagonal with alpha and omega values
    diag(correlations) <- consist_diag
    
    # set row and column names for better table printing
    row.names(correlations) <- c("1. MBI","2. MBI EE","3. MBI DP","4. MBI RPE","5. ERQ","6. ERQ S","7. ERQ R",
                                 "8. SCS","9. NFC","10. DTH","11. DTL","12. DRF","13. COV")
    colnames(correlations) <- c("1","2","3","4","5","6","7","8","9","10","11","12","13")
    
    # create array with p-values of correlation coefficients
    corr_pvalues <- rcorr(as.matrix(score_data[c("mbi","mbi_ee","mbi_dp","mbi_rpe","erq","erq_supp","erq_reap",
                                                 "scs","ncs","dth","dtl","drf","covb")]), type = "spearman")$P
    # replace upper triangle with NA
    corr_pvalues[upper.tri(corr_pvalues, diag = TRUE)] <- NA
    
    # replace p-values with asterisks depending on significance level
    corr_pvalues <- ifelse(is.na(corr_pvalues), "", ifelse(corr_pvalues < .001, "***", ifelse(corr_pvalues < .01, "**", ifelse(corr_pvalues < .05, "*", ""))))
    
    # convert to matrix for the paste()-function
    correlations <- as.matrix(correlations)
    corr_pvalues <- as.matrix(corr_pvalues)
    
    # paste correlations and asterisks together
    correlations <- matrix(paste(correlations, corr_pvalues, sep = ""),
                           nrow = nrow(correlations), dimnames = dimnames(correlations))
    
```

# Results

During visual inspection of correlation plots we noticed an unexpected outlier with very high MBI scores and very low NFC scores.
A Q-Q-plot contrasting Mahalanobis *D^2^* against expected Chi Square values confirmed the outlier.
To adhere to the preregistration, we report the results containing the outlier in this section and the results excluding the outlier in the **Supplementary Material**.

```{r desc_table, echo=FALSE, message=FALSE, warning=FALSE}

  desc_table <-
  kbl(correlations,
      booktabs = T, # no vertical lines
      escape = F, # correctly print special characters
      align = rep("l",14)) %>% # right aligns the text in every column except for the first
    kable_styling(latex_options = c("striped", "scale_down")) %>%
    footnote(general = "MBI = Maslach Burnout Inventory, MBI EE = Emotional exhaustion subscale, MBI DP = Depersonalisation subscale, MBI RPE = Reduced personal efficacy subscale, ERQ = Emotion Regulation Questionnaire, ERQ S = Suppression subscale, ERQ R = Reappraisal subscale, SCS = Self-Control Scale, NFC = Need for Cognition, DTH = Demands Too High, DTL = Demands Too Low, DRF = Demand-Resource-Fit, COV = Covid-19 Burden. \\\\textit{N} = 180. * \\\\textit{p} < .05. ** \\\\textit{p} < .01. *** \\\\textit{p} < .001. Diagonal is Cronbach's Alpha and (in brackets) MacDonald's Omega.",
             escape = F, # correctly print special characters
             threeparttable = T) %>% # line breaks in the footnote
  kableExtra::landscape() # make the table horizontal 

```

## Descriptive statistics

Basic metric descriptives of the questionnaire scores and subscales are listed in **Table 1**.
Only the ERQ sum score and its Reappraisal subscale followed a multivariate normal distribution, so the results of the models should be interpreted with some caution and with a focus on indices that are robust against violation of normality, such as the Satorra-Bentler or Yuan-Bentler-scaled test statistics [@Roesseel2012].
Correlations and internal consistencies are displayed in **Table 2**.
For this descriptive analysis, the variables DTH, DTL, and DRF were computed as a sum of their item scores, not weighted as in the structural equation model.
Using traditional cut-off values [@Nunnally1994], the Cronbach's Alpha of the three demand-resource-ratios can be considered *acceptable*.
The more robust MacDonald's Omega [@Dunn2014] does not deviate much from Cronbach's Alpha and indicates *acceptable* to *good* internal consistency.
As expected, the MBI score was positively correlated with DTH (`r cor_apa(cor.test(score_data$mbi, score_data$dth, method = "spearman"), format = "rmarkdown")`) and negatively with DRF (`r cor_apa(cor.test(score_data$mbi, score_data$drf, method = "spearman"), format = "rmarkdown")`), large associations according the classification scheme by Gignac and Szodorai [-@Gignac2016].
Surprisingly, the correlation between the MBI score and DTL was positive and also large (`r cor_apa(cor.test(score_data$mbi, score_data$dtl, method = "spearman"), format = "rmarkdown")`).
The NFC score correlated negatively with the MBI sum score and about equally with all subscales, contrary to some previous observations in other studies.

`r desc_table`

```{r objective1, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}

  # Replication of Grass et al. (2018)
  
  repli_model <- '
  # direct paths
    scs       ~   a1 * ncs
    erq_reap  ~   a2 * ncs
    erq_supp  ~   a3 * ncs
    mbi_rpe   ~   b1 * scs + b2 * erq_reap + b3 * erq_supp + c * ncs
    
  # indirect effects
    Indirect1 := a1 * b1
    Indirect2 := a2 * b2
    Indirect3 := a3 * b3

  # contrast (if significant, the effects differ)
    Contrast := Indirect1 - Indirect2 - Indirect3
         
  # total effect
    Total := c + (a1 * b1) + (a2 * b2) + (a3 * b3)
'
  
  # Determine the model fit
  
  fit <- sem(
    model = repli_model,
    data  = score_data,
    se = "bootstrap",
    bootstrap = 2000
  )
  
  # Get summary
  
  fit_summ <- summary(fit, fit.measures = TRUE, standardize = TRUE, rsquare = TRUE,
          estimates = TRUE, ci = TRUE)
```

```{r objective1_plot, echo=FALSE, message=FALSE, warning=FALSE}

  # create an array of standardized parameters and an array of significances
  
  fit_param <- fit_summ$PE$std.all[1:7]
  fit_sign <- fit_summ$PE$pvalue[1:7]

  # replace p-values with asterisks depending on significance level
  
  fit_sign <- ifelse(fit_sign < .001, "***", ifelse(fit_sign < .01, "**", ifelse(fit_sign < .05, "*", "")))
  
  # paste correlations and asterisks together
  
  fit_param <- paste(format(round(fit_param, digits = 2), nsmall = 2), fit_sign, sep = "")
  
  # Make a path diagram to visualize the results

  fit_plot <- semPaths(fit, what="std", # display the parameters on the arrows
                       nodeLabels = c("Self-Control", "Reappraisal", "Suppression", "Reduced\nPersonal Efficacy", "Need for Cognition"),
                       layout = matrix(data = c(0,0,0,1,-1,1,-0.3,-1,0.3,0.3), nrow = 5, ncol = 2), # manually set the node locations
                       sizeMan = 24, sizeMan2 = 7, shapeMan = "rectangle", label.scale = FALSE, # specify node properties
                       edgeLabels = fit_param, # specify the edge labels with the asterisks as previously created
                       edge.label.cex = 1, esize = 2, # size specifications
                       fade = FALSE, # do not make insignificant associations invisible
                       residuals = FALSE, # do not plot residuals
                       edge.label.color = "black", posCol = "black", negCol = "black"
)
```

## Replication of Grass et al. (2018)

[Describe mediation results here.]

```{r objective1years, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}

  # Replication of Grass et al. (2018) using years spent teaching as a variable influencing self-control
  # (as indicated by the correlation of it with SCS but not with the other variables)
  
  repli_model_years <- '
  # direct paths
    scs       ~   a1 * ncs + y * years
    erq_reap  ~   a2 * ncs
    erq_supp  ~   a3 * ncs
    mbi_rpe   ~   b1 * scs + b2 * erq_reap + b3 * erq_supp + c * ncs
    
  # indirect effects
    Indirect1 := (a1 * b1) + (y * b1)
    Indirect2 := a2 * b2
    Indirect3 := a3 * b3

  # contrast (if significant, the effects differ)
    Contrast := Indirect1 - Indirect2 - Indirect3
         
  # total effect
    Total := c + (a1 * b1) + (a2 * b2) + (a3 * b3) + (y * b1)
'
  
  # Determine the model fit
  
  fityears <- sem(
    model = repli_model_years,
    data  = score_data,
    se = "bootstrap",
    bootstrap = 2000
  )
  
  # Get summary
  
  fityears_summ <- summary(fityears, fit.measures = TRUE, standardize = TRUE, rsquare = TRUE,
          estimates = TRUE, ci = TRUE)
```

```{r objective1years_plot, echo=FALSE, message=FALSE, warning=FALSE}

  # create an array of standardized parameters and an array of significances
  
  fityears_param <- fityears_summ$PE$std.all[1:8]
  fityears_sign <- fityears_summ$PE$pvalue[1:8]

  # replace p-values with asterisks depending on significance level
  
  fityears_sign <- ifelse(fityears_sign < .001, "***", ifelse(fityears_sign < .01, "**", ifelse(fityears_sign < .05, "*", "")))
  
  # paste correlations and asterisks together
  
  fityears_param <- paste(format(round(fityears_param, digits = 2), nsmall = 2), fityears_sign, sep = "")
  
  # Make a path diagram to visualize the results

  fityears_plot <- semPaths(fityears, what="std", # display the parameters on the arrows
                       nodeLabels = c("Self-Control", "Reappraisal", "Suppression", "Reduced\nPersonal Efficacy", "Need for Cognition", "Years spent\nteaching"),
                       layout = matrix(data = c(0,0,0,1,-1,-1,1,-0.3,-1,0.3,0.3,1), nrow = 6, ncol = 2), # manually set the node locations
                       sizeMan = 24, sizeMan2 = 7, shapeMan = "rectangle", label.scale = FALSE, # specify node properties
                       edge.label.cex = 1, esize = 2, # size specifications
                       edgeLabels = fityears_param, # specify the edge labels with the asterisks as previously created
                       fade = FALSE, # do not make insignificant associations invisible
                       residuals = FALSE, # do not plot residuals
                       exoCov = FALSE, # do not plot covariance between exogenous variables
                       edge.label.color = "black", posCol = "black", negCol = "black"
)
```

[Describe mediation with years-teaching-variable here]

```{r objective2, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}

  # SEM with demand-resource-ratios
  
  sem_model <- '
  # measurement model
    NFC =~ nfc1 + nfc2 + nfc3 + nfc4
    DTH =~ dth1 + dth2 + dth3
    DTL =~ dtl1 + dtl2 + dtl3
    DRF =~ drf1 + drf2 + drf3
    MBI =~ mbi_ee + mbi_dp + mbi_rpe

  # structural model
    DTH ~ a1 * NFC
    DTL ~ a2 * NFC
    DRF ~ a3 * NFC
    MBI ~ c * NFC + b1 * DTH + b2 * DTL + b3 * DRF

  # indirect effects
    Indirect1 := a1 * b1
    Indirect2 := a2 * b2
    Indirect3 := a3 * b3

  # contrast (if significant, the effects differ)
    Contrast := Indirect1 - Indirect2 - Indirect3
         
  # total effect
    Total := c + (a1 * b1) + (a2 * b2) + (a3 * b3)'
  
  # Determine the model fit
  
  fit2 <- sem(
    model = sem_model,
    data  = score_data,
    estimator = "MLR"
  )
  
  # Get summary
  
  fit2_summ <- summary(fit2, fit.measures = TRUE, standardize = TRUE, rsquare = TRUE,
          estimates = TRUE, ci = TRUE)
  
```

```{r objective2_plot, echo=FALSE, message=FALSE, warning=FALSE}

  # create an array of standardized parameters and an array of significances
  
  fit2_param <- fit2_summ$PE$std.all[1:23]
  fit2_sign <- c(rep(1,16), fit2_summ$PE$pvalue[17:23])

  # replace p-values with asterisks depending on significance level
  
  fit2_sign <- ifelse(fit2_sign < .001, "***", ifelse(fit2_sign < .01, "**", ifelse(fit2_sign < .05, "*", "")))
  
  # paste correlations and asterisks together
  
  fit2_param <- paste(format(round(fit2_param, digits = 2), nsmall = 2), fit2_sign, sep = "")  

  # Visualize the results
  
  fit2_plot <- semPaths(fit2, what = "std",
                        nodeLabels = c("nfc1","nfc2","nfc3","nfc4","dth1","dth2","dth3","dtl1","dtl2","dtl3","drf1","drf2","drf3","ee","dp","rpe","NFC","DTH","DTL","DRF","MBI"),
                        layout = matrix(data = c(-1,-0.8,-0.6,-0.4,-0.8,-0.6,-0.4,-0.2,0,0.2,0.4,0.6,0.8,0.6,0.8,1,-0.8,-0.6,0,0.6,0.8,
                                                 -1,-1,-1,-1,1,1,1,1,1,1,1,1,1,-1,-1,-1,-0.5,0.5,0.5,0.5,-0.5),
                                        nrow = 21, ncol = 2),
                        residuals = FALSE, fade = FALSE,
                        edgeLabels = fit2_param, # specify the edge labels with the asterisks as previously created
                        edge.label.cex = 1, esize = 2, # size of the parameters
                        edge.label.color = "black", edge.label.bg = TRUE, # color and background of the parameters
                        posCol = "black", negCol = "black" # color of arrows
  )
```

## Demand-Resource Model

Next we looked at how different ratios of subjective demands and resources affect the association of NFC and burnout in a strictly confirmatory approach.
The parcelling procedure for the indicators of the latent factor NFC resulted in four parcels with a summed difference in average loadings of `r round(ncs_parcels$summed.difference, digits = 3)`.
The first parcel contained item `r ncs_parcels$items[1,1]`, `r ncs_parcels$items[2,1]`, `r ncs_parcels$items[3,1]`, and `r ncs_parcels$items[4,1]`, the second parcel item `r ncs_parcels$items[1,2]`, `r ncs_parcels$items[2,2]`, `r ncs_parcels$items[3,2]`, and `r ncs_parcels$items[4,2]`, the third parcel item `r ncs_parcels$items[1,3]`, `r ncs_parcels$items[2,3]`, `r ncs_parcels$items[3,3]`, and `r ncs_parcels$items[4,3]`, and the fourth parcel item `r ncs_parcels$items[1,4]`, `r ncs_parcels$items[2,4]`, `r ncs_parcels$items[3,4]`, and `r ncs_parcels$items[4,4]`.
Standardized path coefficients of the demand-resource model are illustrated in **Figure 3**.
The robust Chi-square statistic of $\chi^2 = $`r round(fit2@test[["yuan.bentler.mplus"]]$stat, digits = 2)` ($p < .001$) does not indicate good model fit.
However, since it is in the range of `r round(fit2@test[["yuan.bentler.mplus"]]$stat/fit2@test[["yuan.bentler.mplus"]]$df, digits = 0)`$*df < \chi^2 < $`r round(fit2@test[["yuan.bentler.mplus"]]$stat/fit2@test[["yuan.bentler.mplus"]]$df, digits = 0)+1`$*df$ the lack of good fit might be due to the underlying assumption of multivariate normality [@Hu1999; @Schumacker2010], which is violated here.
This also holds true for the RMSEA of `r round(fit2_summ[["FIT"]][["rmsea.robust"]], digits = 2)`, 95% CI [`r round(fit2_summ[["FIT"]][["rmsea.ci.lower.robust"]], digits = 2)`,`r round(fit2_summ[["FIT"]][["rmsea.ci.upper.robust"]], digits = 2)`]


```{r exploratory1, echo=FALSE, message=FALSE, warning=FALSE, eval=FALSE}

  # Exploratory analysis I: MBI subscale instead of sum score
  
  explor1_model <- '
  # measurement model
    NFC =~ nfc1 + nfc2 + nfc3 + nfc4
    DTH =~ dth1 + dth2 + dth3
    DTL =~ dtl1 + dtl2 + dtl3
    DRF =~ drf1 + drf2 + drf3
    RPE =~ mbi_rpe

  # structural model
    DTH ~ a1 * NFC
    DTL ~ a2 * NFC
    DRF ~ a3 * NFC
    RPE ~ c * NFC + b1 * DTH + b2 * DTL + b3 * DRF

  # indirect effects
    Indirect1 := a1 * b1
    Indirect2 := a2 * b2
    Indirect3 := a3 * b3

  # contrast (if significant, the effects differ)
    Contrast := Indirect1 - Indirect2 - Indirect3
         
  # total effect
    Total := c + (a1 * b1) + (a2 * b2) + (a3 * b3)'
  
  # Determine the model fit
  
  fit_explor1 <- sem(
    model = explor1_model,
    data  = score_data,
    estimator = "MLR"
  )
  
  # Get summary
  
  fit_explor1_summ <- summary(fit_explor1, fit.measures = TRUE, standardize = TRUE, rsquare = TRUE,
          estimates = TRUE, ci = TRUE)
```

```{r exploratory2, echo=FALSE, message=FALSE, warning=FALSE, eval=FALSE}

  # Exploratory analysis II: Covid burden as a covariate
  
  # Correlations show that Covid burden is positively associated with MBI and
  # and MBI_EE, and negatively associated with DTH. We will therefore include
  # Covid burden as a mediator between DTH and MBI.
  
  explor2_model <- '
  measurement model
    NFC =~ nfc1 + nfc2 + nfc3 + nfc4
    DTH =~ dth1 + dth2 + dth3
    DTL =~ dtl1 + dtl2 + dtl3
    DRF =~ drf1 + drf2 + drf3
    MBI =~ mbi_ee + mbi_dp + mbi_rpe
    COVB =~ covb

  # structural model
    DTH ~ NFC
    DTL ~ NFC
    DRF ~ NFC
    COVB ~ DTH
    MBI ~ NFC + DTH + DTL + DRF + COVB'
  
  # Determine the model fit
  
  fit_explor2 <- sem(
    model = explor2_model,
    data  = score_data,
    estimator = "MLR"
  )
  
  # Get summary
  
  fit_explor2_summ <- summary(fit_explor2, fit.measures = TRUE, standardize = TRUE, rsquare = TRUE,
          estimates = TRUE, ci = TRUE)
```


# Discussion


\newpage

# References

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id="refs" custom-style="Bibliography"></div>
\endgroup
